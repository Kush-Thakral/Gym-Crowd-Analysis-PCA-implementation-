{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gym Crowdedness Analysis with PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Objective : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To **predict** how crowded a university gym would be at a given time of day (and some other features, including weather)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Data Decription : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of 26,000 people counts (about every 10 minutes) over one year. The dataset also contains information about the weather and semester-specific information that might affect how crowded it is. The label is the number of people, which has to be predicted given some subset of the features.\n",
    "\n",
    "**Label**:\n",
    "\n",
    "- Number of people\n",
    "\n",
    "**Features**:\n",
    "\n",
    "     1. date (string; datetime of data)\n",
    "     2. timestamp (int; number of seconds since beginning of day)\n",
    "     3. dayofweek (int; 0 [monday] - 6 [sunday])\n",
    "     4. is_weekend (int; 0 or 1) [boolean, if 1, it's either saturday or sunday, otherwise 0]\n",
    "     5. is_holiday (int; 0 or 1) [boolean, if 1 it's a federal holiday, 0 otherwise]\n",
    "     6. temperature (float; degrees fahrenheit)\n",
    "     7. isstartof_semester (int; 0 or 1) [boolean, if 1 it's the beginning of a school semester, 0 otherwise]\n",
    "     8. month (int; 1 [jan] - 12 [dec])\n",
    "     9. hour (int; 0 - 23)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> # Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model would be built and PCA would be implemented in the following way : \n",
    "\n",
    "- **Data Cleaning and PreProcessing**\n",
    "- **Exploratory Data Analysis :**\n",
    "  \n",
    "      - Uni-Variate Analysis : Histograms , Distribution Plots\n",
    "      - Bi-Variate Analysis : Pair Plots\n",
    "      - Correlation Matrix\n",
    "      \n",
    "- **Processing :**\n",
    "      \n",
    "      - OneHotEncoding \n",
    "      - Feature Scaling : Standard Scaler\n",
    "\n",
    "- **Splitting Dataset** \n",
    "- **Principal Component Analysis**\n",
    "- **Modelling : Random Forest**\n",
    "\n",
    "      - Random forest without PCA\n",
    "      - Random Forest with PCA\n",
    "      \n",
    "- **Conclusion**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `1` Data Cleaning and PreProcessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries and loading Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "61e8090a-db25-b4c6-3635-514217f78bb4"
   },
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "2682c5c2-b2ab-d65f-9d39-6fd4b96d075a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>number_people</th>\n",
       "      <th>date</th>\n",
       "      <th>timestamp</th>\n",
       "      <th>day_of_week</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>temperature</th>\n",
       "      <th>is_start_of_semester</th>\n",
       "      <th>is_during_semester</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>2015-08-14 17:00:11-07:00</td>\n",
       "      <td>61211</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>45</td>\n",
       "      <td>2015-08-14 17:20:14-07:00</td>\n",
       "      <td>62414</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>2015-08-14 17:30:15-07:00</td>\n",
       "      <td>63015</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>44</td>\n",
       "      <td>2015-08-14 17:40:16-07:00</td>\n",
       "      <td>63616</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>45</td>\n",
       "      <td>2015-08-14 17:50:17-07:00</td>\n",
       "      <td>64217</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>71.76</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   number_people                       date  timestamp  day_of_week  \\\n",
       "0             37  2015-08-14 17:00:11-07:00      61211            4   \n",
       "1             45  2015-08-14 17:20:14-07:00      62414            4   \n",
       "2             40  2015-08-14 17:30:15-07:00      63015            4   \n",
       "3             44  2015-08-14 17:40:16-07:00      63616            4   \n",
       "4             45  2015-08-14 17:50:17-07:00      64217            4   \n",
       "\n",
       "   is_weekend  is_holiday  temperature  is_start_of_semester  \\\n",
       "0           0           0        71.76                     0   \n",
       "1           0           0        71.76                     0   \n",
       "2           0           0        71.76                     0   \n",
       "3           0           0        71.76                     0   \n",
       "4           0           0        71.76                     0   \n",
       "\n",
       "   is_during_semester  month  hour  \n",
       "0                   0      8    17  \n",
       "1                   0      8    17  \n",
       "2                   0      8    17  \n",
       "3                   0      8    17  \n",
       "4                   0      8    17  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv(r'C:\\Users\\kusht\\OneDrive\\Desktop\\Excel-csv\\PCA analysis.csv') #Replace it with your path where the data file is stored\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Print the `info()` of the dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Describe the dataset using `describe()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "c7ae0580-f900-4153-631f-069d0f242e35"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "bf7a5858-5f8e-5279-508f-1e571dc14f19"
   },
   "source": [
    "**TASK : Convert temperature in farenheit into celsius scale using the formula `Celsius=(Fahrenheit-32)* (5/9)`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (~1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Convert the timestamp into hours in 12 h format as its currently in seconds and drop `date` coulmn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE: (~ 1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `2` Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2.1` Uni-Variate and Bi-Variate Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Pair Plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Use `pairplot()` to make different pair scatter plots of the entire dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE :\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: Now analyse scatter plots between `number_people` and all other attributes using a `for loop` to properly know what are the ideal conditions for people to come to the gym** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE \n",
    "    \n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse the plots and understand :**\n",
    "1. **At what time , temperature , week of the day more people come in?**\n",
    "        \n",
    "2. **Whether people like to come to the gym in a holiday or a weekend or they prefer to come to gym during working days?**\n",
    "       \n",
    "3. **Which month is most preferable for people to come to the gym?** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Distribution Plots**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot individual `distplot()` for `temperature` and `number_people` to check out the individual distribution of the attributes** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE : \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `2.2` Correlation Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot a correlation matrix and make it more understandable using `sns.heatmap`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE : \n",
    "\n",
    "\n",
    "### END CODE HERE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyse the correlation matrix and understand the different dependencies of attributes on each other** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `3.` Processing : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.1` One hot encoding :\n",
    "One hot encoding certain attributes to not give any ranking/priority to any instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: One Hot Encode following attributes `month` , `hour` , `day of week`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## YOU CAN USE EITHER get_dummies() OR OneHotEncoder()\n",
    "\n",
    "### START CODE HERE : \n",
    "\n",
    "### END CODE "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `3.2` Feature Scaling :\n",
    "Some attributes ranges are ver different compared to other values and during PCA implementation this might give a problem thus you need to standardise some of the attributes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK: Using `StandardScaler()` , standardise `temperature` and `timestamp`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "## You can use two individual scalers one for temperature and other for timestamp\n",
    "## you can use an array type data=df.values and standradise data then split data into X and y\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "### START CODE HERE : (Replace places having '#' with the code)\n",
    "data=df.values\n",
    "scaler1 = StandardScaler()\n",
    "scaler1.fit(#) # for timestamp\n",
    "data[#] = scaler1.transform(#)\n",
    "\n",
    "scaler2 = StandardScaler()\n",
    "scaler2.fit(data[#]) # for temperature\n",
    "data[#] = scaler2.transform(data[#])\n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `4.` Splitting the dataset : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Split the dataset into dependent and independent variables and name them y and X respectively** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE : \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Split the X ,y into training and test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "### START CODE HERE : \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `5.` Principal Component Analysis \n",
    "\n",
    "Principal component analysis (PCA) is a technique for reducing the dimensionality of such datasets, increasing interpretability but at the same time minimizing information loss. It does so by creating new uncorrelated variables that successively maximize variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How does it work? :**\n",
    "\n",
    "- First, a matrix is calculated that summarizes how our variables all relate to one another.\n",
    "\n",
    "- Secondly , The matrix is broken down into two separate components: direction and magnitude. so its easy to understand the “directions” of the data and its “magnitude” (or how “important” each direction is). The photo below, displays the two main directions in this data: the “red direction” and the “green direction.” In this case, the “red direction” is the more important one as given how the dots are arranged, “red direction”  comprises most of the data and thus is s more important than the “green direction” (Hint: Think of  What would fitting a line of best fit to this data look like?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/832/1*P8_C9uk3ewpRDtevf9wVxg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Then the data is transformed to align with these important directions (which are combinations of our original variables). The photo below is the same exact data as above, but transformed so that the x- and y-axes are now the “red direction” and “green direction.”  What would the line of best fit look like here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1400/1*V3JWBvxB92Uo116Bpxa3Tw.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So PCA tries to find the most important directions in which most of the data is spread and thus reduces it to those components thereby reducing the number of attributes to train and increasing computational speed. A 3D example is given below : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/1024/1*vfLvJF8wHaQjDaWv6Mab2w.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above a 3D plot is reduced to a 2d plot still retaining most of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that you have understood this , lets try to implement it** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Print the PCA fit_transform of X(independent variables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "985295c0-1ef4-5ddb-a048-e85cc21e2360"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "### START CODE HERE : (Replace spaces having '#' with the code)\n",
    "pca = PCA()\n",
    "pca.fit_transform(#)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Get covariance using `get_covariance()`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "37a76d04-29fe-9a20-4d0c-a686a18cca29"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~ 1 line of code) \n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Get explained variance using `explained_variance_ratio`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "c7f9ec22-472e-4458-2d85-02c24bc92a33"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE : \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot a bar graph of `explained variance`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "a836f786-1fb1-9cf2-24ba-b48ff82f9c68"
   },
   "outputs": [],
   "source": [
    "# you can use plt.bar()\n",
    "\n",
    "### START CODE HERE : (Replace spaces having '#' with the code)\n",
    "with plt.style.context('dark_background'):\n",
    "    plt.figure(figsize=(15,12))\n",
    "\n",
    "    plt.bar(range(49), '#', alpha=0.5, align='center',\n",
    "            label='individual explained variance')\n",
    "    plt.ylabel('#')\n",
    "    plt.xlabel('#')\n",
    "    plt.legend(loc='best')\n",
    "    plt.tight_layout()\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2e8f750b-b7e3-f5aa-0a26-048739a4ee70"
   },
   "source": [
    "**Analyse the plot and estimate how many componenets you want to keep**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a `PCA()` object with n_components =20 and fit-transform in the dataset (X) and assign to a new variable `X_new`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_cell_guid": "4bf9dda2-bd49-123a-b210-a0eebde4e143"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE : \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , `X_new` is the dataset for PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Get Covariance using `get_covariance`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "5e24ed72-560f-ea4d-8666-828a08433d6a"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE (~1 Line of code)\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Get the explained variance using `explained_variance_ratio`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_cell_guid": "f0912408-e4fc-fecc-3c31-65d94105eca5"
   },
   "outputs": [],
   "source": [
    "### START CODE HERE :\n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Plot bar plot of `exlpained variance`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_cell_guid": "9dff6023-b064-81a7-b2d7-fbee96ec70c4"
   },
   "outputs": [],
   "source": [
    "# You can use plt.bar()\n",
    "\n",
    "### START CODE HERE: \n",
    "    \n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `6.` Modelling : Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To understand Random forest classifier , lets first get a brief idea about Decision Trees in general. Decision Trees are very intuitive and at everyone have used this knowingly or unknowingly at some point . Basically the model keeps sorting them into categories forming a large tree by responses of some questons (decisions) and thats why its called decision tree. An image example would help understand it better :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/960e89743476577bd696b3ac16885cf1e1d19ad1/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313030302f312a4c4d6f4a6d584373516c6369475445796f534e3339672e6a706567\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Random Forest` : Random forest, like its name implies, consists of a large number of individual decision trees that operate as an [ensemble](https://en.wikipedia.org/wiki/Ensemble_learning) . Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://camo.githubusercontent.com/30aec690ddc10fa0ae5d3135d0c7a6b745eb5918/68747470733a2f2f6d69726f2e6d656469756d2e636f6d2f6d61782f313030302f312a56484474566144504e657052676c49417637324246672e6a706567\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fundamental concept is large number of relatively uncorrelated models (trees) operating as a committee will outperform any of the individual constituent models. Since this dataset has very low correlation between attributes , random forest can be a good option."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section you'll have to make a random forest model and train it on both without PCA dataset and with PCA datset to analyse the differences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `6.1` Random Forest Without PCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a random forest model and train it on without PCA training set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "aeebfae2-3b4e-f06a-c120-7e71736566b7"
   },
   "outputs": [],
   "source": [
    "# Establish model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "47f50275-827a-8941-996d-6df5684b54fb"
   },
   "outputs": [],
   "source": [
    "# Try different numbers of n_estimators and print the scores\n",
    "# You can use a variable estimators = np.arrange(10,200,10) and then a for loop to take all the values of estimators\n",
    "\n",
    "### START CODE HERE : (Replace spaces having '#' with code)\n",
    "estimators = np.arange(10, 200, 10)\n",
    "scores = []\n",
    "for n in estimators:\n",
    "    model.set_params(n_estimators='#')\n",
    "    model.fit('#', '#')\n",
    "    scores.append(model.score(X_test, y_test))\n",
    "print(scores)    \n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a plot between `n_estimator` and `scores` to properly get the best number of estimators**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "6f65d7b5-9a32-a330-0516-7bc96bf59bfd"
   },
   "outputs": [],
   "source": [
    "## Use plt.plot\n",
    "\n",
    "### START CODE HERE : \n",
    "\n",
    "### END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `6.2` Random Forest With PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Split the your dataset with PCA into training and testing set using `train_test_split`** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_cell_guid": "3af5a41f-c928-8af7-6068-cbed122e82fb"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "### START CODE HERE  :\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a random forest model called `model_pca` and fit it into the new X_train and y_train and then print out the random forest scores for dataset with PCA applied to it**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "cf626a49-27f4-f251-9947-7efef301a670"
   },
   "outputs": [],
   "source": [
    "# Establish model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "model_pca = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "_cell_guid": "bc9c5949-9c05-9864-0612-24b0b5904fa3"
   },
   "outputs": [],
   "source": [
    "# You can use different number of estimators\n",
    "# # You can use a variable estimators = np.arrange(10,200,10) and then a for loop to take all the values of estimators\n",
    "\n",
    "### START CODE HERE : \n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TASK : Make a plot between `n_estimator` and `score` and find the best parameter** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "_cell_guid": "65834f80-dac5-656a-12b9-11b4fbe3f6c5"
   },
   "outputs": [],
   "source": [
    "# you can use plt.plot\n",
    "### START CODE HERE : \n",
    "\n",
    "\n",
    "### END CODE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This completes modelling and now its time to analyse your models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `7.` Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse the plots and find the best n_estimator. you can also hypertune other parameter using GridSearchCV or Randomised search. Also understand whether using PCA was beneficial or not , if not try to justify it. "
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
